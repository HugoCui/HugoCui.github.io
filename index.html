<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />







<title>Hugo Cui</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  .centerImage
  {
   text-align:center;
   display:block;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
.toggle-trigger {

padding: 1px;
background: white;
font-family: Futura, serif;
padding-bottom: 1px;
margin: 0;
cursor: pointer;
color:#fa625f;
}

.toggle-trigger:hover {
border-bottom: 1px #fa625f dotted;
margin-bottom: -2px;
}

.toggle-wrap {
padding: 1px;
display: none;
background: white;
font-family: Futura, serif;
padding-bottom: 8px;
margin: 0;
}



</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"> </a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Hugo Cui</a>
</li>
<li>
  <a href="Talks.html">Talks & teaching</a>
</li>



      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:hugo.cui@epfl.ch">
    <i class="fa fa-envelope" style="font-size:16px"></i>   
  </a>
</li>
<li>
  <a href="https://github.com/HugoCui">
    <i style="font-size:19px" class="fab fa-github"></i> 
  </a>
</li>
<li>
  <a href="https://scholar.google.com/citations?user=A2M_k-wAAAAJ&hl=fr">
    <i class="ai ai-google-scholar"  style="font-size:20px"></i>
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/hugo-cui-4a3a4217b/">
    <i class="fab fa-linkedin"  style="font-size:20px"></i>
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">
</div>



<p><br><br> <img src="circle.jpg" class="centerImage"  /></p>

<p><br><br> PhD student @ <a href="https://www.epfl.ch/labs/spoc/">SPOC lab</a> at EPFL, advised by Lenka Zdeborová. I hold
  a Master degree in theoretical physics from ENS Paris. My research lies at the crossroads
  of machine learning theory, high-dimensional statistics, and statistical physics. I am interested in 
understanding the mechanisms of learning in high dimensions, and the information-theoretic limits thereof.</p>
<p><br><br></p>


<div id="recent-news" class="section level3">
  <center><h3>News</h3></center>
  <p><br><br></p>
<ul>
<li><p>[<i style="color:blue;">Sept. 2023</i>] Our sharp analysis of <a href="https://arxiv.org/pdf/2305.11041.pdf">denoising auto-encoders</a> 
  has been accepted in NeurIPS 2023 as a spotlight!</p></li>
<li><p>[<i style="color:blue;">Aug. 2023</i>] Our paper on <a href="https://arxiv.org/pdf/2201.12655.pdf">error scaling laws</a> 
in kernel classification has been published in MLST!</p></li>
<li><p>[<i style="color:blue;">Jul. 2023</i>] Presenting our analysis of error rates 
in kernel regression and classification @ the 5<sup>th</sup> workshop on Neural Scaling Laws in Honolulu </p></li>
<li><p>[<i style="color:blue;">Jul. 2023</i>] Presenting my latest work on Bayesian learning and deep random features @ ICML 2023</p></li>
</ul>
</div>

<p><br><br></p>


<div id="Publications" class="section level3">
  <center><h3>Publications</h3></center>
  <p><br><br></p>
<ul>
  <li>
    <p>
        <i>High-dimensional Asymptotics of Denoising Autoencoders, </i>
        <b>
          Hugo Cui,
        </b>
        Lenka Zdeborová <i style="color:green;">NeurIPS 2023 (spotlight).</i>
        [<a href="https://arxiv.org/pdf/2305.11041.pdf">arXiv</a>
        ] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear
autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where
the number of training samples and the input dimension jointly tend to infinity while the number of
hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared
test error. Building on this result, we quantitatively characterize the advantage of the considered
architecture over the autoencoder without the skip connection that relates closely to principal
component analysis. We further show that our results accurately capture the learning curves on a
range of real data sets.

                </div>
    </p></li>
    <li>
      <p>
          <i>Error rates for kernel classification under source and capacity conditions, </i>
          <b>
            Hugo Cui,
          </b>
          Bruno Loureiro, Florent Krzakala, Lenka Zdeborová <i style="color:green;">MLST 2023.</i>
          [<a href="https://arxiv.org/pdf/2201.12655.pdf">arXiv</a>
          ] 
          <span class="toggle-trigger">
              [Show Abstract]
              <span>
                  <div class="toggle-wrap">
                      <b>Abstract:</b>
                      In this manuscript, we consider the problem of kernel classification. Works on kernel regression
have shown that the rate of decay of the prediction error with the number of samples for a large
class of data-sets is well characterized by two quantities: the capacity and source of the data-set.
In this work, we compute the decay rates for the misclassification (prediction) error under the
Gaussian design, for data-sets satisfying source and capacity assumptions. We derive the rates as a
function of the source and capacity coefficients for two standard kernel classification settings, namely
margin-maximizing Support Vector Machines (SVM) and ridge classification, and contrast the two
methods. As a consequence, we find that the known worst-case rates are loose for this class of
data-sets. Finally, we show that the rates presented in this work are also observed on real data-sets.

                  </div>
      </p></li>

<li>
  <p>
      <i>Bayes-optimal learning of 
        deep random networks of extensive-width, </i>
      <b>
        Hugo Cui,
      </b>
      Florent Krzakala, Lenka Zdeborová, <i style="color:green;">ICML 2023 (Oral).</i>
      [<a href="https://arxiv.org/pdf/2302.00375.pdf">arXiv</a>
      ] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  We consider the problem of learning a target function corresponding to a deep, 
                  extensive-width, non-linear neural network with random Gaussian weights. 
                  We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large and propose a closed-form expression 
                  for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of 
                  ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, 
                  achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically 
                  that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test 
                  error close to zero from quadratically many samples.
              </div>
  </p></li>


<li>
  <p>
      <i>Deterministic equivalent and error universality of deep random features learning </i>
      Dominik Schröder<sup>*</sup>,
      <b>
        Hugo Cui<sup>*</sup>,
      </b>
      Daniil Dmitriev, Bruno Loureiro <i style="color:green;">ICML 2023.</i>
      [<a href="https://arxiv.org/pdf/2302.00401.pdf">arXiv</a>
      ] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  This manuscript considers the problem of learning a random Gaussian 
                  network function using a fully connected network with frozen intermediate layers and trainable readout layer. 
                  This problem can be seen as a natural generalization of the widely studied random features model to deeper architectures. 
                  First, we prove Gaussian universality of the test error in a ridge regression setting where the learner and target networks share the same intermediate layers, 
                  and provide a sharp asymptotic formula for it. Establishing this result requires proving a deterministic equivalent for traces of the deep 
                  random features sample covariance matrices which can be of independent interest.
                    Second, we conjecture the asymptotic Gaussian universality of the test error in the more
                    general setting of arbitrary convex losses and generic learner/target architectures. 
                    We provide extensive numerical evidence for this conjecture, which requires the derivation of closed-form expressions 
                    for the layer-wise post-activation population covariances. In light of our results, we investigate the interplay 
                    between architecture design and implicit regularization.
              </div>
  </p></li>



<li>
  <p>
      <i>Large deviations of semisupervised learning in the stochastic block model, </i>
      
      <b>
        Hugo Cui,
      </b>
      Luca Saglietti, Lenka Zdeborová <i style="color:green;">PRE 2022.</i>
      [<a href="https://arxiv.org/pdf/2108.00847.pdf">arXiv</a>
      ] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  In semisupervised community detection, the membership of a set of revealed nodes is known in addition to the graph structure 
                  and can be leveraged to achieve better inference accuracies. While previous works investigated the case where the revealed nodes are selected at random, 
                  this paper focuses on correlated subsets leading to atypically high accuracies. In the framework of the dense stochastic block model, 
                  we employ statistical physics methods to derive a large deviation analysis of the number of these rare subsets, as characterized by their free energy. 
                  We find theoretical evidence of a nonmonotonic relationship between reconstruction accuracy and the free energy 
                  associated to the posterior measure of the inference problem. We further discuss possible implications for active learning applications in community detection.
              </div>
  </p></li>


<li>
  <p>
      <i>Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime, </i>
      
      <b>
        Hugo Cui,
      </b>
      Bruno Loureiro, Florent Krzakala, Lenka Zdeborová <i style="color:green;">NeurIPS 2021 & Jstat Mech Special Issue 2022.</i>
      [<a href="https://arxiv.org/abs/2105.15004">arXiv</a>
      ] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  In this manuscript we consider Kernel Ridge Regression (KRR) under the Gaussian design. Exponents for 
                  the decay of the excess generalization error of KRR have been reported in various works under the assumption of power-law decay of eigenvalues of 
                  the features co-variance. These decays were, however, provided for sizeably different setups, namely in the noiseless case with constant regularization 
                  and in the noisy optimally regularized case. Intermediary settings have been left substantially uncharted. In this work, 
                  we unify and extend this line of work, providing characterization of all regimes and excess error decay rates that can be observed in terms of 
                  the interplay of noise and regularization. In particular, we show the existence of a transition in the noisy setting between the noiseless 
                  exponents to its noisy values as the sample complexity is increased. Finally, we illustrate how this crossover can also be observed on real data sets.
              </div>
  </p></li>

<li>
  <p>
      <i>Learning curves of generic features maps for realistic datasets with a teacher-student model, </i>
      Bruno Loureiro, Cédric Gerbelot,
      <b>
        Hugo Cui,
      </b>
      Sebastian Goldt, Marc Mézard,Florent Krzakala, Lenka Zdeborová <i style="color:green;">NeurIPS 2021.</i>
      [<a href="https://arxiv.org/abs/2102.08127">arXiv</a>
      ] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  Teacher-student models provide a framework in which the typical-case performance of high-dimensional 
                  supervised learning can be described in closed form. The assumptions of Gaussian i.i.d. input data underlying the
                   canonical teacher-student model may, however, be perceived as too restrictive to capture the behaviour of realistic data sets. 
                   In this paper, we introduce a Gaussian covariate generalisation of the model where the teacher and student can act on different spaces, 
                   generated with fixed, but generic feature maps. While still solvable in a closed form, this generalization is able 
                   to capture the learning curves for a broad range of realistic data sets, thus redeeming the potential of the teacher-student framework. 
                   Our contribution is then two-fold: First, we prove a rigorous formula for the asymptotic training loss and generalisation error. 
                   Second, we present a number of situations where the learning curve of the model captures the one of a realistic data set learned with 
                   kernel regression and classification, with out-of-the-box feature maps such as random projections or scattering transforms, 
                   or with pre-learned ones - such as the features learned by training multi-layer neural networks. We discuss both the power and the limitations of the framework.
              </div>
  </p></li>

  <li>
    <p>
        <i>Large deviations for the perceptron model and consequences for active learning, </i>
        <b>
          Hugo Cui
        </b>
        Luca Saglietti, Lenka Zdeborová <i style="color:green;">MSML 2020 & MLST 2021.</i>
        [<a href="https://iopscience.iop.org/article/10.1088/2632-2153/abfbbb/pdf">arXiv</a>
        ] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    Active learning (AL) is a branch of machine learning that deals with problems where unlabeled
data is abundant yet obtaining labels is expensive. The learning algorithm has the possibility of
querying a limited number of samples to obtain the corresponding labels, subsequently used for
supervised learning. In this work, we consider the task of choosing the subset of samples to be
labeled from a fixed finite pool of samples. We assume the pool of samples to be a random matrix
and the ground truth labels to be generated by a single-layer teacher random neural network. We
employ replica methods to analyze the large deviations for the accuracy achieved after supervised
learning on a subset of the original pool. These large deviations then provide optimal achievable
performance boundaries for any AL algorithm. We show that the optimal learning performance
can be efficiently approached by simple message-passing AL algorithms. We also provide a
comparison with the performance of some other popular active learning strategies.

                </div>
    </p></li>



</ul>
</div>



<p><br><br></p>
<div id="Preprints" class="section level3">
  <center><h3>Preprints</h3></center>
  <p><br><br></p>
<ul>


</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
