<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />







<title>Hugo Cui</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  .centerImage
  {
   text-align:center;
   display:block;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
.toggle-trigger {

padding: 1px;
background: white;
font-family: Futura, serif;
padding-bottom: 1px;
margin: 0;
cursor: pointer;
color:#ff3b3b;
}

.toggle-trigger:hover {
border-bottom: 1px #fa625f dotted;
margin-bottom: -2px;
}

.toggle-wrap {
padding: 1px;
display: none;
background: white;
font-family: Futura, serif;
padding-bottom: 8px;
margin: 0;
}



</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"> </a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Hugo Cui</a>
</li>
<li>
  <a href="Talks.html">Talks & teaching</a>
</li>
<li>
  <a href="https://hugocui.github.io/CV.pdf">CV</a>
</li>



      </ul>
      <ul class="nav navbar-nav navbar-right">

<li>
  <a href="https://github.com/HugoCui">
    <i style="font-size:19px" class="fab fa-github"></i> 
  </a>
</li>
<li>
  <a href="https://scholar.google.com/citations?user=A2M_k-wAAAAJ&hl=fr">
    <i class="ai ai-google-scholar"  style="font-size:20px"></i>
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/hugo-cui-4a3a4217b/">
    <i class="fab fa-linkedin"  style="font-size:20px"></i>
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">
</div>



<p><br><br> <img src="circle.jpg" class="centerImage"  /></p>

<p><br><br> I am a postdoctoral fellow at the Center for Mathematical Sciences and Applications (<a href="https://cmsa.fas.harvard.edu/">CMSA</a>) at Harvard University. Prior to that,
   I received my PhD from EPFL, where I was advised by <a href="https://www.epfl.ch/labs/spoc/">Lenka Zdeborová </a>. I hold
  a Master degree in theoretical physics from ENS Paris. My research lies at the crossroads
  of machine learning theory, high-dimensional statistics, and statistical physics. I am interested in 
understanding the mechanisms of learning in high dimensions, and the information-theoretic limits thereof.</p>


<p>The best way to reach me is usually by email, at hugo [at] cmsa [dot] fas [dot] harvard [dot] edu </p>
<p><br><br></p>


<div id="recent-news" class="section level3">
  <center><h3>News</h3></center>
  <p><br><br></p>
<ul>
<li><p>[<i style="color:green;">Jan. 2025</i>] New preprint out ! In our <a href="https://arxiv.org/abs/2501.03937"> latest work</a>,
  we sharply characterize the distribution of samples generated by a diffusion model.</p></li>
  <li><p>[<i style="color:orange;">Sept. 2024</i>] Interested in how statistical physics ideas can be used
  to analyze neural networks ? My <a href="https://arxiv.org/abs/2409.13904"> new review</a>
should be of interest to computer scientists wanting to learn of such tools, and to physicists curious of
how these techniques apply to machine learning.</p></li>
<li><p>[<i style="color:blue;">Jul. 2024</i>] Learn more about our work on <a href="https://arxiv.org/pdf/2402.03902.pdf"> phase transitions in attention models</a> at the HiLD workshop in ICML
! Our works on <a href="https://arxiv.org/pdf/2402.03902.pdf"> structured feature maps</a>
and <a href="https://arxiv.org/pdf/2402.04980.pdf"> spiked random features</a> can be found in the main 
conference track.</p></li>
<li><p>[<i style="color:red;">Feb. 2024</i>] Three new preprints out! (1/3) In <a href="https://arxiv.org/pdf/2402.03902.pdf"> the first work</a>,
    we investigate a solvable model of attention mechanism, and evidence a phase transition from positional to semantic learning.</p></li>
<li><p>[<i style="color:red;">Feb. 2024</i>] (2/3) In <a href="https://arxiv.org/pdf/2402.04980.pdf"> the second work</a>, 
      we sharply characterized how feature learning arises in shallow networks after a single gradient step.</p></li>
<li><p>[<i style="color:red;">Feb. 2024</i>] (3/3) Finally, in <a href="https://arxiv.org/abs/2402.13999"> the last work</a>, 
        we explore the learning of deep random networks with structured weights.</p></li>
<li><p>[<i style="color:green;">Jan. 2024</i>] I will be presenting <a href="https://arxiv.org/abs/2310.03575">our work on flow-based models</a>
    at ICLR 2024 in Vienna. </p></li>
<li><p>[<i style="color:orange;">Oct. 2023</i>] New preprint out! In <a href="https://arxiv.org/abs/2310.03575">our latest work</a>
we conduct a sharp analysis of a generative model learning from a limited number of samples. </p></li>
<li><p>[<i style="color:blue;">Sept. 2023</i>] Our sharp analysis of <a href="https://arxiv.org/pdf/2305.11041.pdf">denoising auto-encoders</a> 
  has been accepted in NeurIPS 2023 as a spotlight!</p></li>
<li><p>[<i style="color:blue;">Aug. 2023</i>] Our paper on <a href="https://arxiv.org/pdf/2201.12655.pdf">error scaling laws</a> 
in kernel classification has been published in MLST!</p></li>
<li><p>[<i style="color:red;">Jul. 2023</i>] Presenting our analysis of error rates 
in kernel regression and classification @ the 5<sup>th</sup> workshop on Neural Scaling Laws in Honolulu </p></li>
<li><p>[<i style="color:red;">Jul. 2023</i>] Presenting my latest work on Bayesian learning and deep random features @ ICML 2023</p></li>
</ul>
</div>

<p><br><br></p>


<div id="Publications" class="section level3">
  <center><h3>Publications</h3></center>
  <p><br><br></p>
<ul>
  <li>
    <p>
        <i>A phase transition between positional and semantic learning in a solvable model of dot-product attention, </i>
        <br>
        <b>
          Hugo Cui,
        </b>
        Freya Behrens, Florent Krzakala, Lenka Zdeborová, <i style="color:green;"> NeurIPS 2024 (<b>spotlight</b>)</i>
        <br>
        [<a href="https://arxiv.org/pdf/2402.03902.pdf">arXiv</a>] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    We investigate how a dot-product attention layer learns a positional attention matrix 
                    (with tokens attending to each other based on their respective positions) and a semantic attention matrix
                     (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally 
                     show how the same simple architecture can learn to implement a solution using either the positional or
                      semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer 
                      with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data 
                      and a comparably large number of training samples, we provide a closed-form characterization of the global 
                      minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a 
                      positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter 
                      with increasing sample complexity. Finally, we compare the dot-product attention layer to linear positional 
                      baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to 
                      sufficient data.

                </div>
    </p></li>

  <li>
    <p>
        <i>Asymptotics of feature learning in two-layer networks after one gradient-step, </i>
        <br>
        <b>
          Hugo Cui,
        </b>
        Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborová, Bruno Loureiro, <i style="color:green;"> ICML 2024 (<b>spotlight</b>).</i>
        <br>
        [<a href="https://arxiv.org/pdf/2402.04980.pdf">arXiv</a>] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    In this manuscript we investigate the problem of how two-layer neural networks learn features from data,
                     and improve over the kernel regime, after being trained with a single gradient descent step. 
                     Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on 
                     Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization 
                     error in the high-dimensional limit where the number of samples , the width  and the input dimension  grow at
                      a proportional rate. We characterize exactly how adapting to the data is crucial for the network to 
                      efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can 
                      only express linear functions in this regime. To our knowledge, our results provides the first tight 
                      description of the impact of feature learning in the generalization of two-layer neural networks in the 
                      large learning rate regime , beyond perturbative finite width corrections of the conjugate and neural 
                      tangent kernels.

                </div>
    </p></li>

    <li>
      <p>
          <i>Asymptotics of Learning with Deep Structured (Random) Features, </i>
          <br>
          Dominik Schröder, Daniil Dmitriev,
          <b>
            Hugo Cui,
          </b>
          Bruno Loureiro, <i style="color:green;"> ICML 2024.</i>
          <br>
          [<a href="https://arxiv.org/abs/2402.13999">arXiv</a>] 
          <span class="toggle-trigger">
              [Show Abstract]
              <span>
                  <div class="toggle-wrap">
                      <b>Abstract:</b>
                      For a large class of feature maps we provide a tight asymptotic characterisation of the 
                      test error associated with learning the readout layer, in the high-dimensional limit where 
                      the input dimension, hidden layer widths, and number of training samples are proportionally large. 
                      This characterization is formulated in terms of the population covariance of the features. 
                      Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, 
                      namely deep non-linear fully-connected networks with random but structured weights, 
                      whose row-wise covariances are further allowed to depend on the weights of previous layers. 
                      For such networks we also derive a closed-form formula for the feature covariance in terms of the 
                      weight matrices. We further find that in some cases our results can capture feature maps learned by deep, 
                      finite-width neural networks trained under gradient descent.
  
                  </div>
      </p></li>


  <li>
    <p>
        <i>Analysis of learning a flow-based generative model from limited sample complexity, </i><br>
        <b>
          Hugo Cui,
        </b>
        Florent Krzakala, Eric Vanden-Eijnden and Lenka Zdeborová, <i style="color:green;"> ICLR 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2310.03575">arXiv</a>] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, 
                    to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem.
                     First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a 
                     shallow denoising auto-encoder trained on a finite number n of samples from the target distribution. 
                     Building on this analysis, we provide a sharp description of the corresponding generative flow, 
                     which pushes the base Gaussian density forward to an approximation of the target density. 
                     In particular, we provide closed-form formulae for the distance between the mean of the generated mixture 
                     and the mean of the target mixture, which we show decays as 1/n. Finally, 
                     this rate is shown to be in fact Bayes-optimal.

                </div>
    </p></li>
  <li>
    <p>
        <i>High-dimensional Asymptotics of Denoising Autoencoders, </i><br>
        <b>
          Hugo Cui,
        </b>
        Lenka Zdeborová, <i style="color:green;">NeurIPS 2023 (<b>spotlight</b>).</i>
        <br>
        [<a href="https://arxiv.org/pdf/2305.11041.pdf">arXiv</a>] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear
autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where
the number of training samples and the input dimension jointly tend to infinity while the number of
hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared
test error. Building on this result, we quantitatively characterize the advantage of the considered
architecture over the autoencoder without the skip connection that relates closely to principal
component analysis. We further show that our results accurately capture the learning curves on a
range of real data sets.

                </div>
    </p></li>
    <li>
      <p>
          <i>Error rates for kernel classification under source and capacity conditions, </i><br>
          <b>
            Hugo Cui,
          </b>
          Bruno Loureiro, Florent Krzakala, Lenka Zdeborová, <i style="color:green;">MLST 2023.</i>
          <br>
          [<a href="https://arxiv.org/pdf/2201.12655.pdf">arXiv</a>] 
          <span class="toggle-trigger">
              [Show Abstract]
              <span>
                  <div class="toggle-wrap">
                      <b>Abstract:</b>
                      In this manuscript, we consider the problem of kernel classification. Works on kernel regression
have shown that the rate of decay of the prediction error with the number of samples for a large
class of data-sets is well characterized by two quantities: the capacity and source of the data-set.
In this work, we compute the decay rates for the misclassification (prediction) error under the
Gaussian design, for data-sets satisfying source and capacity assumptions. We derive the rates as a
function of the source and capacity coefficients for two standard kernel classification settings, namely
margin-maximizing Support Vector Machines (SVM) and ridge classification, and contrast the two
methods. As a consequence, we find that the known worst-case rates are loose for this class of
data-sets. Finally, we show that the rates presented in this work are also observed on real data-sets.

                  </div>
      </p></li>

<li>
  <p>
      <i>Bayes-optimal learning of 
        deep random networks of extensive-width, </i><br>
      <b>
        Hugo Cui,
      </b>
      Florent Krzakala, Lenka Zdeborová, <i style="color:green;">ICML 2023 (<b>Oral</b>).</i>
      <br>
      [<a href="https://arxiv.org/pdf/2302.00375.pdf">arXiv</a>] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  We consider the problem of learning a target function corresponding to a deep, 
                  extensive-width, non-linear neural network with random Gaussian weights. 
                  We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large and propose a closed-form expression 
                  for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of 
                  ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, 
                  achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically 
                  that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test 
                  error close to zero from quadratically many samples.
              </div>
  </p></li>


<li>
  <p>
      <i>Deterministic equivalent and error universality of deep random features learning, </i><br>
      Dominik Schröder<sup>*</sup>,
      <b>
        Hugo Cui<sup>*</sup>,
      </b>
      Daniil Dmitriev, Bruno Loureiro, <i style="color:green;">ICML 2023.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2302.00401.pdf">arXiv</a>] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  This manuscript considers the problem of learning a random Gaussian 
                  network function using a fully connected network with frozen intermediate layers and trainable readout layer. 
                  This problem can be seen as a natural generalization of the widely studied random features model to deeper architectures. 
                  First, we prove Gaussian universality of the test error in a ridge regression setting where the learner and target networks share the same intermediate layers, 
                  and provide a sharp asymptotic formula for it. Establishing this result requires proving a deterministic equivalent for traces of the deep 
                  random features sample covariance matrices which can be of independent interest.
                    Second, we conjecture the asymptotic Gaussian universality of the test error in the more
                    general setting of arbitrary convex losses and generic learner/target architectures. 
                    We provide extensive numerical evidence for this conjecture, which requires the derivation of closed-form expressions 
                    for the layer-wise post-activation population covariances. In light of our results, we investigate the interplay 
                    between architecture design and implicit regularization.
              </div>
  </p></li>



<li>
  <p>
      <i>Large deviations of semisupervised learning in the stochastic block model, </i><br>
      
      <b>
        Hugo Cui,
      </b>
      Luca Saglietti, Lenka Zdeborová, <i style="color:green;">PRE 2022.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2108.00847.pdf">arXiv</a>] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  In semisupervised community detection, the membership of a set of revealed nodes is known in addition to the graph structure 
                  and can be leveraged to achieve better inference accuracies. While previous works investigated the case where the revealed nodes are selected at random, 
                  this paper focuses on correlated subsets leading to atypically high accuracies. In the framework of the dense stochastic block model, 
                  we employ statistical physics methods to derive a large deviation analysis of the number of these rare subsets, as characterized by their free energy. 
                  We find theoretical evidence of a nonmonotonic relationship between reconstruction accuracy and the free energy 
                  associated to the posterior measure of the inference problem. We further discuss possible implications for active learning applications in community detection.
              </div>
  </p></li>


<li>
  <p>
      <i>Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime, </i><br>
      
      <b>
        Hugo Cui,
      </b>
      Bruno Loureiro, Florent Krzakala, Lenka Zdeborová, <i style="color:green;">NeurIPS 2021 & Jstat Mech Special Issue 2022.</i>
      <br>
      [<a href="https://arxiv.org/abs/2105.15004">arXiv</a>] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  In this manuscript we consider Kernel Ridge Regression (KRR) under the Gaussian design. Exponents for 
                  the decay of the excess generalization error of KRR have been reported in various works under the assumption of power-law decay of eigenvalues of 
                  the features co-variance. These decays were, however, provided for sizeably different setups, namely in the noiseless case with constant regularization 
                  and in the noisy optimally regularized case. Intermediary settings have been left substantially uncharted. In this work, 
                  we unify and extend this line of work, providing characterization of all regimes and excess error decay rates that can be observed in terms of 
                  the interplay of noise and regularization. In particular, we show the existence of a transition in the noisy setting between the noiseless 
                  exponents to its noisy values as the sample complexity is increased. Finally, we illustrate how this crossover can also be observed on real data sets.
              </div>
  </p></li>

<li>
  <p>
      <i>Learning curves of generic features maps for realistic datasets with a teacher-student model, </i><br>
      Bruno Loureiro, Cédric Gerbelot,
      <b>
        Hugo Cui,
      </b>
      Sebastian Goldt, Marc Mézard,Florent Krzakala, Lenka Zdeborová, <i style="color:green;">NeurIPS 2021.</i>
      <br>
      [<a href="https://arxiv.org/abs/2102.08127">arXiv</a>] 
      <span class="toggle-trigger">
          [Show Abstract]
          <span>
              <div class="toggle-wrap">
                  <b>Abstract:</b>
                  Teacher-student models provide a framework in which the typical-case performance of high-dimensional 
                  supervised learning can be described in closed form. The assumptions of Gaussian i.i.d. input data underlying the
                   canonical teacher-student model may, however, be perceived as too restrictive to capture the behaviour of realistic data sets. 
                   In this paper, we introduce a Gaussian covariate generalisation of the model where the teacher and student can act on different spaces, 
                   generated with fixed, but generic feature maps. While still solvable in a closed form, this generalization is able 
                   to capture the learning curves for a broad range of realistic data sets, thus redeeming the potential of the teacher-student framework. 
                   Our contribution is then two-fold: First, we prove a rigorous formula for the asymptotic training loss and generalisation error. 
                   Second, we present a number of situations where the learning curve of the model captures the one of a realistic data set learned with 
                   kernel regression and classification, with out-of-the-box feature maps such as random projections or scattering transforms, 
                   or with pre-learned ones - such as the features learned by training multi-layer neural networks. We discuss both the power and the limitations of the framework.
              </div>
  </p></li>

  <li>
    <p>
        <i>Large deviations for the perceptron model and consequences for active learning, </i><br>
        <b>
          Hugo Cui,
        </b>
        Luca Saglietti, Lenka Zdeborová, <i style="color:green;">MSML 2020 & MLST 2021.</i>
        <br>
        [<a href="https://iopscience.iop.org/article/10.1088/2632-2153/abfbbb/pdf">arXiv</a>] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    Active learning (AL) is a branch of machine learning that deals with problems where unlabeled
data is abundant yet obtaining labels is expensive. The learning algorithm has the possibility of
querying a limited number of samples to obtain the corresponding labels, subsequently used for
supervised learning. In this work, we consider the task of choosing the subset of samples to be
labeled from a fixed finite pool of samples. We assume the pool of samples to be a random matrix
and the ground truth labels to be generated by a single-layer teacher random neural network. We
employ replica methods to analyze the large deviations for the accuracy achieved after supervised
learning on a subset of the original pool. These large deviations then provide optimal achievable
performance boundaries for any AL algorithm. We show that the optimal learning performance
can be efficiently approached by simple message-passing AL algorithms. We also provide a
comparison with the performance of some other popular active learning strategies.

                </div>
    </p></li>



</ul>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://infoscience.epfl.ch/entities/publication/b9197d03-f3ae-494d-829a-bb213d647815"><b>PhD thesis</b></a> : <i>Topics in statistical physics of high-dimensional machine learning </i>, <b>Hugo Cui</b>, 2024
</div>



<p><br><br></p>
<div id="Preprints" class="section level3">
  <center><h3>Preprints</h3></center>
  <p><br><br></p>
<ul>
  <li>
    <p>
        <i>High-dimensional learning of narrow neural networks,</i><br>
        <b>
          Hugo Cui,
        </b>
       <i style="color:green;"> preprint</i>
       <br>
        [<a href="https://arxiv.org/pdf/2409.13904">arXiv</a>] 
        <span class="toggle-trigger">
            [Show Abstract]
            <span>
                <div class="toggle-wrap">
                    <b>Abstract:</b>
                    Recent years have been marked with the fast-pace diversification and increasing ubiquity of machine learning
applications. Yet, a firm theoretical understanding of the surprising efficiency of neural networks to learn from highdimensional data still proves largely elusive. In this endeavour, analyses inspired by statistical physics have proven
instrumental, enabling the tight asymptotic characterization of the learning of neural networks in high dimensions, for
a broad class of solvable models. This manuscript reviews the tools and ideas underlying recent progress in this line
of work. We introduce a generic model – the sequence multi-index model–, which encompasses numerous previously
studied models as special instances. This unified framework covers a broad class of machine learning architectures
with a finite number of hidden units – including multi-layer perceptrons, autoencoders, attention mechanisms–, and
tasks –(un)supervised learning, denoising, contrastive learning–, in the limit of large data dimension, and comparably
large number of samples. We explicate in full detail the analysis of the learning of sequence multi-index models, using
statistical physics techniques such as the replica method and approximate message-passing algorithms. This manuscript
thus provides a unified presentation of analyses reported in several previous works, and a detailed overview of central
techniques in the field of statistical physics of machine learning. This review should be a useful primer for machine
learning theoreticians curious of statistical physics approaches; it should also be of value to statistical physicists interested
in the transfer of such ideas to the study of neural networks.

                </div>
    </p></li>
<li>
<p>
<i>A random matrix theory perspective on the spectrum of learned features and asymptotic generalization capabilities,</i><br>
Yatin Dandi,Luca Pesce, 
<b>
Hugo Cui,
</b>
Florent Krzakala, Yue M Lu, Bruno Loureiro,
<i style="color:green;"> preprint</i>
<br>
[<a href="https://arxiv.org/pdf/2410.18938">arXiv</a>] 
<span class="toggle-trigger">
[Show Abstract]
<span>
  <div class="toggle-wrap">
      <b>Abstract:</b>
A key property of neural networks is their capacity of adapting to data during training. 
Yet, our current mathematical understanding of feature learning and its relationship to generalization remain limited.
In this work, we provide a random matrix analysis of how fully-connected two-layer neural networks adapt to the 
target function after a single, but aggressive, gradient descent step. We rigorously establish the equivalence between
the updated features and an isotropic spiked random feature model, in the limit of large batch size. For the latter
model, we derive a deterministic equivalent description of the feature empirical covariance matrix in terms of 
certain low-dimensional operators. This allows us to sharply characterize the impact of training in the asymptotic
feature spectrum, and in particular, provides a theoretical grounding for how the tails of the feature spectrum
  modify with training. The deterministic equivalent further yields the exact asymptotic generalization error, 
  shedding light on the mechanisms behind its improvement in the presence of feature learning. Our result goes 
  beyond standard random matrix ensembles, and therefore we believe it is of independent technical interest.
  Different from previous work, our result holds in the challenging maximal learning rate regime, is fully 
  rigorous and allows for finitely supported second layer initialization, which turns out to be crucial for
    studying the functional expressivity of the learned features. This provides a sharp description of the
    impact of feature learning in the generalization of two-layer neural networks, beyond the random features
      and lazy training regimes.

  </div>
</p></li>

<li>
  <p>
  <i>A precise asymptotic analysis of learning diffusion models: theory and insights,</i><br>
  <b>
  Hugo Cui,
  </b>
  Cengiz Pehlevan, Yue M Lu,
  <i style="color:green;"> preprint</i>
  <br>
  [<a href="https://arxiv.org/pdf/2501.03937">arXiv</a>] 
  <span class="toggle-trigger">
  [Show Abstract]
  <span>
    <div class="toggle-wrap">
        <b>Abstract:</b>
        In this manuscript, we consider the problem of learning a flow or diffusion-based generative model parametrized by a two-layer auto-encoder, 
        trained with online stochastic gradient descent, on a high-dimensional target density with an underlying low-dimensional manifold structure. 
        We derive a tight asymptotic characterization of low-dimensional projections of the distribution of samples generated by the learned model,
         ascertaining in particular its dependence on the number of training samples. Building on this analysis, we discuss how mode collapse can arise, 
         and lead to model collapse when the generative model is re-trained on generated synthetic data.
    </div>
  </p></li>

    
</ul>
</div>

<p><br><br></p>
<div id="recent-news" class="section level3">
  <center><h3>Awards</h3></center>
  <p><br><br></p>
<ul>
<li><p><a href="https://actu.epfl.ch/news/2024-physics-doctoral-thesis-award/">EPFL Physics doctoral thesis award</a> 2024.</p></li>
<li><p><a href="https://actu.epfl.ch/news/2024-physics-doctoral-thesis-award/">EPFL best 8% PhD distinction</a> in Physics.</p></li>
<li><p><a href="https://www.gresearch.com/news/g-research-2024-phd-prize-winners-epfl/">G-Research PhD prize in mathematics and data science</a>, EPFL 3rd prize.</p></li>
<li><p><a href="https://www.famelab.ch/">Famelab</a> international science communication competition, Switzerland national winner, and international finalist (representing Switzerland).</p></li>
<li><p> ENS Paris 2016 national competitive entrance exam, 1st place.</p></li>
<li><p> Recipient of two <a href="https://www.academie-sciences.fr/fr/Laureats/laureats-2014-prix-thematiques.html">thematic awards</a> from the French
Académie des Sciences (for (Inter)national chemistry olympiads)</p></li>
</ul>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
